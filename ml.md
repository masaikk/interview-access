# 百面机器学习



## 特征工程

### 数据归一化的意义？$\bigstar$

对于两个范围不同的变量来说，把它们的变化范围拉到相似的范围，在梯度下降的时候更容易找到最优解。

### word2vec $\bigstar \bigstar$

分为**CBOW**和**Skip-gram**

CBOW是将上下文的词语来预测当前词，Skip-gram是当前词来预测上下文各词的概率。

在输入层中，总共N个不同的单词转化为N个独热向量表示。在隐含层中，有K个隐含单元，所以权重矩阵N\*K。输出层矩阵K\*N，之后连接softmax层。

softmax公式
$$
P(y=w_n|x)=\frac{e^{x_n}}{\sum\limits_{k=1}^{N}e^{x_{k}}}
$$
一个词为N维原始输入向量，x表示为一个N维的输出向量，$x_{n}$表示为在原始输出向量中，与单词$w_{n}$对应维度的取值。

## 模型评估

### 评估指标的局限性$ \bigstar$

准确率$accuracy=\frac{被正确分类的样本个数}{总样本个数}$。这种导致在不同类的样本比例不平均的时候，占比大的样本更加能影响准确率。小比例的类别，就算全部预测错误，对总体准确率也影响不大。

召回率$recall=\frac{分类正确的正样本个数}{全部正样本个数}$

只用某个点对应的精准率和召回率是不能完全地衡量模型的性能，只能通过P-R曲线的整体表现来判断。此外，F1 score和ROC曲线也可以反映一个排序模型的性能。


$$
F1=\frac{2*accuracy*recall}{accuracy+recall}
$$


较大的错误值可能会导致总体平方根误差变得很大，其中，平方根误差为$RMSE=\sqrt{\frac{\sum\limits_{i=1}^{n}{(第i个样本点真实值-第i个样本点预测值)^2}}{n}}$

这种计算方法如果存在严重的离群点，会被严重影响。解决办法有三：

+ 把这些点当作噪声忽略掉

+ 如果不考虑这些是噪声点，就应该对这些点重新机制建模

+ 使用更合适的指标，比如评价绝对百分比误差MAPE(mean absolute percent error)

  
  $$
  MAPE=\sum\limits_{i=1}^{n}{\abs{\frac{第i个样本点真实值-第i个样本点预测值}{第i个样本点真实值}}*{\frac{100}{n}}}
  $$
  
  这里相当于把每个点都进行了归一化操作。
  


### ROC曲线

Receiver Operating Characteristic Curve(ROC)

横坐标为假阳率（False Positive Rate, FPR)$FPR=\frac{FP负样本被预测成正样本的个数}{N真实的负样本的个数}$；纵坐标为真阳率（Ture Positive Rate,TPR)$TPR=\frac{TP预测对的正样本的个数}{P真实的正样本的个数}$



**AUC**指对着ROC的横坐标做积分，求面积。



### A/B测试

*为什么要进行线上的A/B测试？*

+ 离线评估的结果无法完全消除过拟合的影响。
+ 离线评估无法替代线上的生产环境。离线评估不会考虑延迟，数据丢失，标签数据丢失等情况。
+ 线上的商业指标无法在离线评估中得到计算。

*如何使用A/B测试？答案待考*



### 过拟合与欠拟合

*降低过拟合的方法？*

+ 从数据入手，获取更多的训练数据。
+ 降低模型的复杂度，比如说减少神经元个数，层数，降低树的深度，剪枝等等。
+ 正则化的方法。
+ 集成学习，比如说Bagging方法。

*降低欠拟合的方法？*

+ 添加新特征，比如因子分解机，梯度提升决策树，Deep-Crossing。
+ 增加模型的复杂度。
+ 减小正则化系数。

## 经典算法

决策树的启发函数

+ ID3 最大信息增益

  H(D)为数据集D的经验熵，H(D|A)为某个特征A对于数据集D的经验条件熵。信息增益就是
  $$
  g(D,A)=H(D)-H(D|A)
  $$

+ C4.5 最大信息增益比

  特征A对于数据集D的最大信息增益比为
  $$
  g_{R}(D,A)=\frac{g(D,A)}{H_A(D)}
  $$
  这里的$H_A(D)$为数据集关于A的取值熵。

+ CART 最大基尼指数（Gini）

  它描述的是信息的纯度
  $$
  Gini(D)=1=\sum\limits_{k=1}^{n}(\frac{\abs{C_k}}{\abs{D}})^2
  $$

决策树的剪枝

+ 预剪枝
  1. 当树达到一定的深度，停止生长。
  2. 当当前节点样本数小于某个阈值，停止生长。
  3. 每次分裂计算对数据集的精准度的提升，小于某个阈值则停止生长。
+ 后剪枝
  + 之后再补
  + P67



## 降维

**好难不会，之后再补。P74-P88**

## 非监督学习

### K-means算法的具体步骤：

1. 数据预处理：归一化，离群点处理。
2. 随机选取K个簇的中心。
3. 计算代价函数，对于每个点x到每个簇的中心点的距离影响的函数。
4. 迭代n轮，重复下列操作直到收敛：
   1. 对于每一个样本点x，计算并且分配到它最近的簇的中心点。
   2. 对于每一个类簇，重新计算中心点。

### K-means算法的优缺点及其调优：

+ 缺点：
  + 受初始值和离群点的影响。
  + 结果只是局部最优解。
  + 无法解决簇的大小相差过大的问题。
  + 不太适用于离散分类。
  + 样本只能被划分到单一的类中
+ 优点：
  + 计算复杂度优秀，O(NKt)，N为数据的数目，K为簇的个数，t为迭代轮数。
+ 调优：P95
  + 数据归一化和离群点处理。
  + 合理选择K值。
  + 采用核函数。

### K-means算法的改进：

+ K-means++ 算法：

  比较与普通kmeans随机选择K个簇的中心，在改进的算法中，选择了n个点之后，距离当前n个点越远的点越容易被选为第n+1个点。

+ ISODATA：

  P98 待续

### 高斯混合模型：

多个符合高斯分布的数据的混合的模型。

设置参数$\mu_{i}$和$\sigma_{i}$以及权重或者生成数据的概率$\pi_{i}$

则高斯分布的公式为
$$
p(x)=\sum\limits_{i=1}^{K}\pi_{i}N(x|\mu_{i},\sigma_{i})
$$
与K-means算法相同的是，也是依靠了EM算法，即是在最大化目标函数时，先固定一个变量使整体函数变为凸优化函数，求导得到最值，然后利用最优参数更新被固定的变量，然后是下一个循环。P104（这里可以着重看看）

### 自组织映射神经网络：

P106看不懂

### 聚类算法的评估：

主要是三个任务：

+ 估计聚类趋势
+ 判定数据簇数
+ 判定聚类质量

## 概率图模型

### 生成式模型和判别式模型的区别

假设可观测的变量集合为X，需要预测的变量集合为Y，其他变量集合为Z。

生成式模型是对联合概率分布$P(X,Y,Z)$进行建模，即$P(Y|X)=\frac{P(X,Y)}{P(X)}=\frac{\sum_{Z}P(X,Y,Z)}{\sum_{Y,Z}P(X,Y,Z)}$。

判别式模型是对条件概率分布$P(Y,Z|X)$进行建模，然后消除掉无关的变量Z就能得到对Y的预测，即$P(Y|X)=\sum_{Z}P(Y,Z|X)$。

常见的生成式模型有朴素贝叶斯，贝叶斯网络，pLSA，LDA，隐马尔可夫模型。

最大熵模型和条件随机场是判别模型。

### 隐马尔可夫

模型描述：P128

+ 概率计算问题：已知模型的所有参数，计算观测序列Y出现的概率，可使用前向和后向算法求解。
+ 预测问题：已知模型的所有参数和观测序列Y，计算最可能的隐状态序列X，可以使用维特比算法。
+ 学习问题：已知观测序列Y，求解使得该观测序列概率最大的模型参数。可以使用Baum-Welch算法。

### 主题模型

P133

主要包括pLSA和LDA

## 优化算法

主要是梯度下降的优化器，原理问题。P160
